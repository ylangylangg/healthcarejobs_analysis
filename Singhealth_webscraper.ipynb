{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1 (startrow=0)...\n",
      "Found 25 jobs on this page\n",
      "Fetching page 2 (startrow=25)...\n",
      "Found 25 jobs on this page\n",
      "Fetching page 3 (startrow=50)...\n",
      "Found 25 jobs on this page\n",
      "Fetching page 4 (startrow=75)...\n",
      "Found 25 jobs on this page\n",
      "Fetching page 5 (startrow=100)...\n",
      "Found 25 jobs on this page\n",
      "Fetching page 6 (startrow=125)...\n",
      "Found 25 jobs on this page\n",
      "Fetching page 7 (startrow=150)...\n",
      "Found 25 jobs on this page\n",
      "Fetching page 8 (startrow=175)...\n",
      "Found 25 jobs on this page\n",
      "Fetching page 9 (startrow=200)...\n",
      "Found 25 jobs on this page\n",
      "Fetching page 10 (startrow=225)...\n",
      "Found 25 jobs on this page\n",
      "Fetching page 11 (startrow=250)...\n",
      "Found 25 jobs on this page\n",
      "Fetching page 12 (startrow=275)...\n",
      "Found 25 jobs on this page\n",
      "Fetching page 13 (startrow=300)...\n",
      "Found 25 jobs on this page\n",
      "Fetching page 14 (startrow=325)...\n",
      "Found 25 jobs on this page\n",
      "Fetching page 15 (startrow=350)...\n",
      "Found 25 jobs on this page\n",
      "Fetching page 16 (startrow=375)...\n",
      "Found 25 jobs on this page\n",
      "Fetching page 17 (startrow=400)...\n",
      "Found 25 jobs on this page\n",
      "Fetching page 18 (startrow=425)...\n",
      "Found 18 jobs on this page\n",
      "Fetching page 19 (startrow=450)...\n",
      "Found 0 jobs on this page\n",
      "No more jobs found, stopping pagination.\n",
      "✅ 443 job listings copied to clipboard in Excel-friendly format!\n",
      "Paste into Excel and the data will automatically appear in columns with clickable links.\n",
      "✅ 443 job listings saved to singhealth_jobs.csv\n",
      "Open in Excel and you'll see:\n",
      "- Proper columns for Job Title, Hospital, and Link\n",
      "- Clickable hyperlinks in the 'Excel Link' column\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import csv\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pyperclip\n",
    "import time\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Base URL to scrape\n",
    "BASE_URL = \"https://careers.singhealth.com.sg/search/?createNewAlert=false&q=&optionsFacetsDD_customfield3=\"\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "DELAY = 1  # seconds between requests \n",
    "\n",
    "def fetch_jobs(startrow=0, max_pages=23):\n",
    "    \"\"\"Scrape job postings from the SingHealth careers page with pagination support.\"\"\"\n",
    "    jobs = []\n",
    "    seen_links = set()\n",
    "    page_count = 0\n",
    "    \n",
    "    while True:\n",
    "        # Build URL with current pagination parameter\n",
    "        url = f\"{BASE_URL}&startrow={startrow}\"\n",
    "        print(f\"Fetching page {page_count + 1} (startrow={startrow})...\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Failed to retrieve the webpage (status code: {response.status_code})\")\n",
    "                break\n",
    "                \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            job_listings = soup.find_all('div', class_='job-listing') or soup.find_all('li', class_='job-item')\n",
    "            \n",
    "            # If the above selectors don't work, fall back to a more general approach\n",
    "            if not job_listings:\n",
    "                job_listings = soup.find_all('a', href=True)\n",
    "            \n",
    "            current_page_jobs = 0\n",
    "            \n",
    "            for job in job_listings:\n",
    "                job_link = job if job.name == 'a' else job.find('a', href=True)\n",
    "                \n",
    "                if not job_link or 'href' not in job_link.attrs or \"job\" not in job_link['href'].lower():\n",
    "                    continue\n",
    "                    \n",
    "                link = urljoin(\"https://careers.singhealth.com.sg\", job_link['href'])\n",
    "                \n",
    "                if link in seen_links:\n",
    "                    continue\n",
    "                    \n",
    "                seen_links.add(link)\n",
    "                title = job_link.text.strip()\n",
    "                \n",
    "                # Extract hospital name (if available)\n",
    "                hospital = \"Unknown\"\n",
    "                match = re.search(r'\\((.*?)\\)', title)\n",
    "                if match:\n",
    "                    hospital = match.group(1)\n",
    "                    # Remove hospital from title if found in parentheses\n",
    "                    title = title.replace(f\"({hospital})\", \"\").strip()\n",
    "                \n",
    "                jobs.append({\n",
    "                    \"Job Title\": title, \n",
    "                    \"Hospital\": hospital, \n",
    "                    \"Link\": link,\n",
    "                    \"Excel Link\": f'=HYPERLINK(\"{link}\", \"View Job\")'  # Excel formula for clickable link\n",
    "                })\n",
    "                current_page_jobs += 1\n",
    "            \n",
    "            print(f\"Found {current_page_jobs} jobs on this page\")\n",
    "            \n",
    "            # Check if we should continue to next page\n",
    "            if current_page_jobs == 0:\n",
    "                print(\"No more jobs found, stopping pagination.\")\n",
    "                break\n",
    "                \n",
    "            # Increment for next page (assuming 25 items per page - adjust if needed)\n",
    "            startrow += 25\n",
    "            page_count += 1\n",
    "            \n",
    "            # Stop if we've reached the maximum requested pages\n",
    "            if max_pages is not None and page_count >= max_pages:\n",
    "                print(f\"Reached maximum page limit of {max_pages}.\")\n",
    "                break\n",
    "                \n",
    "            # Be polite and don't overwhelm the server\n",
    "            time.sleep(DELAY)\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed: {e}\")\n",
    "            break\n",
    "            \n",
    "    return jobs\n",
    "\n",
    "def copy_jobs_to_clipboard(jobs):\n",
    "    \"\"\"Copy job listings in Excel-friendly format to clipboard.\"\"\"\n",
    "    if not jobs:\n",
    "        print(\"No jobs found to copy.\")\n",
    "        return\n",
    "        \n",
    "    # Create Excel-friendly TSV (tab-separated) format\n",
    "    excel_data = \"Job Title\\tHospital\\tLink\\tExcel Link\\n\"\n",
    "    \n",
    "    for job in jobs:\n",
    "        # Escape special characters and ensure proper formatting\n",
    "        title = job['Job Title'].replace('\"', '\"\"')\n",
    "        hospital = job['Hospital'].replace('\"', '\"\"')\n",
    "        excel_data += f'\"{title}\"\\t\"{hospital}\"\\t\"{job[\"Link\"]}\"\\t{job[\"Excel Link\"]}\\n'\n",
    "    \n",
    "    pyperclip.copy(excel_data)\n",
    "    print(f\"✅ {len(jobs)} job listings copied to clipboard in Excel-friendly format!\")\n",
    "    print(\"Paste into Excel and the data will automatically appear in columns with clickable links.\")\n",
    "\n",
    "def save_jobs_to_excel(jobs, filename=\"singhealth_jobs.csv\"):\n",
    "    \"\"\"Save job listings to a CSV file with Excel-friendly formatting.\"\"\"\n",
    "    if not jobs:\n",
    "        print(\"No jobs found to save.\")\n",
    "        return\n",
    "        \n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"Job Title\", \"Hospital\", \"Link\", \"Excel Link\"])\n",
    "        writer.writeheader()\n",
    "        \n",
    "        for job in jobs:\n",
    "            # Clean up data for CSV output\n",
    "            cleaned_job = {\n",
    "                \"Job Title\": job['Job Title'],\n",
    "                \"Hospital\": job['Hospital'],\n",
    "                \"Link\": job['Link'],\n",
    "                \"Excel Link\": job['Excel Link']\n",
    "            }\n",
    "            writer.writerow(cleaned_job)\n",
    "    \n",
    "    print(f\"✅ {len(jobs)} job listings saved to {filename}\")\n",
    "    print(\"Open in Excel and you'll see:\")\n",
    "    print(\"- Proper columns for Job Title, Hospital, and Link\")\n",
    "    print(\"- Clickable hyperlinks in the 'Excel Link' column\")\n",
    "\n",
    "def save_jobs_to_csv(jobs, filename=\"singhealth_jobs.csv\"):\n",
    "    \"\"\"Save job listings to a plain CSV file.\"\"\"\n",
    "    if not jobs:\n",
    "        print(\"No jobs found to save.\")\n",
    "        return\n",
    "        \n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"Job Title\", \"Hospital\", \"Link\"])\n",
    "        writer.writeheader()\n",
    "        \n",
    "        for job in jobs:\n",
    "            writer.writerow({\n",
    "                \"Job Title\": job['Job Title'],\n",
    "                \"Hospital\": job['Hospital'],\n",
    "                \"Link\": job['Link']\n",
    "            })\n",
    "    \n",
    "    print(f\"✅ {len(jobs)} job listings saved to {filename} (plain CSV)\")\n",
    "\n",
    "# Fetch jobs with pagination\n",
    "# Set max_pages=None to scrape all available pages, or set to a specific number\n",
    "jobs = fetch_jobs(startrow=0, max_pages=23)  # Example: scrape first 3 pages\n",
    "\n",
    "if jobs:\n",
    "    copy_jobs_to_clipboard(jobs)\n",
    "    save_jobs_to_excel(jobs)\n",
    "else:\n",
    "    print(\"No jobs found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
